{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a pretty interesting project. It took me around one month to finish. Here I will explain some of the difficulties I met and how they are overcomed. I will also explained how the training model was built and the Keras structure I used for the model.\n",
    "\n",
    "1. The most difficult thing at the beginning of the project is I have no idea whether it's the problem of my training data or  my Keras model when the car was only running a few seconds and crash on the roadside of the simulator. These problem was overcomed step by step by improving both of the training data part as well as the Keras model part. For this project I have to say, both the feeding data and the Keras architecture are critical to the final result of the model. \n",
    "\n",
    "2. I use a regular Toshiba laptop for the training without a Graph card. For models with 100, 000 parameters it took around 20 min to train 10 EPOCHS and for models with 200, 000 parameters it took around one hour. The training process indeed took me a lot of time, but I could also use the training time to do a lot of side work such as designing the next Keras model. \n",
    "\n",
    "3. I only use the sample data provided by the Udacity for training. I do have a joystick, and I tried both the stable and Beta simulators, however, finally I gave up abtaining my own training data. The reason is because whenever I use my own training data or I merged my training data with the sample data, the driving model I got would perform worse than when I use the sample data only. It actually took me more than 10 hours of time for obtaining the training data, but I'm not a good game player, I couldn't control the throttle very well, and turning the corners especially the last two in track 1 was painful at such a high speed. I hope in a real self drving car project I don't have to be the driver by myself.\n",
    "\n",
    "4. I use the sample data only, but of course only the existing sample data are not enough for obtaining a good model as I have tried the sample data over all the model I have designed and it always failed at the last two corners of Track 1. As a result, the side camera images were included in the training data, and serveral image augmentations, such as changing brightness, flipping, adding random shadow and transforming image within certain range were employed to generate more data for training. Please be noted the side camera method and the image augementation method were copied and modified from the webpage https://chatbotslife.com/using-augmentation-to-mimic-human-driving-496b569760a9#.lx3y9nl5a. Details will be explained in the main body.\n",
    "\n",
    "5. A data generator was used for generating data. The generator keeps generating 32 data randomly from the training data per batch. As the training data contains too many zero steering angles which might cause the car having a tendency of driving straigt forward, a keeper was used to filter out part of the data with low steering angles. The idea of the keeper was also from the webpage https://chatbotslife.com/using-augmentation-to-mimic-human-driving-496b569760a9#.lx3y9nl5a and it was modified to accormodate my own code.\n",
    "\n",
    "6. The Keras model I used is the same as the Nvidia paper, with ELU added into each layer to introduce nonlinearity into the model. I also tried several other models by adding different layers such as MaxPooling2D, AveragePooling2D. It turns out that most of the model I designed worked well on the first 3/4 portion of the track, but the car just can't turn over the first corner after the bridge. It seems like most model I designed couldn't extact enough features from the images right after the bridge. If we check those images carefully, we could find in that corner the right line break off for several meters, which becomes the biggest obstacle hindering the success of most models. Anyway, after testing all different layers I find the Nvidia model with ELU() layers is efficient in training the data, and finally wil; give the acceptable result. The idea of introducing ELU was from the webpage of https://chatbotslife.com/learning-human-driving-behavior-using-nvidias-neural-network-model-and-image-augmentation-80399360efee#.4pikhnmrs. The data in the model was normalized into (-1, 1) first. \n",
    "\n",
    "7. I tried to resize the images into different sizes. My experience about the image size is that larger size images keep more useful informations and are better for feature extraction. For example, the model using resize of 32×64 images can't pass the bridge which might because it didn't extract the feature for the road on the bridge very well. The model using 48×64 images could pass the bridge with no problem, however, it always failed the first corner after the bridge. It seems like the model has difficulties in extracting enough features that the right line was missing in the corner. Finally I choose to use the 66×200 as the image size, which might provide more information for the model to extract. Of course, the model layers has to be adjusted in order to fit different image sizes, thus, changing the image size actually brings more changes to the model than just changing the size only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import base64\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy import ndimage\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import scipy.misc\n",
    "from io import BytesIO\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augment_brightness_camera_images(image):\n",
    "    image1 = cv2.cvtColor(image,cv2.COLOR_RGB2HSV)\n",
    "    random_bright = 0.25 + np.random.uniform()\n",
    "    image1[:,:,2] = image1[:,:,2]*random_bright\n",
    "    image1 = cv2.cvtColor(image1,cv2.COLOR_HSV2RGB)\n",
    "    return image1\n",
    "\n",
    "def trans_image(image,steer,trans_range):\n",
    "    rows, cols = image.shape[:2]\n",
    "    tr_x = trans_range*np.random.uniform()-trans_range/2\n",
    "    steer_ang = steer + tr_x*0.0033\n",
    "    tr_y = 40*np.random.uniform()-40/2\n",
    "    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])\n",
    "    image_tr = cv2.warpAffine(image,Trans_M,(cols,rows))\n",
    "    return image_tr,steer_ang\n",
    "\n",
    "def add_random_shadow(image):\n",
    "    top_y = 320*np.random.uniform()\n",
    "    top_x = 0\n",
    "    bot_x = 160\n",
    "    bot_y = 320*np.random.uniform()\n",
    "    image_hls = cv2.cvtColor(image,cv2.COLOR_RGB2HLS)\n",
    "    shadow_mask = 0*image_hls[:,:,1]\n",
    "    X_m = np.mgrid[0:image.shape[0],0:image.shape[1]][0]\n",
    "    Y_m = np.mgrid[0:image.shape[0],0:image.shape[1]][1]\n",
    "    shadow_mask[((X_m-top_x)*(bot_y-top_y) -(bot_x - top_x)*(Y_m-top_y) >=0)]=1\n",
    "    \n",
    "    if np.random.randint(2)==1:\n",
    "        random_bright = .5\n",
    "        cond1 = shadow_mask==1\n",
    "        cond0 = shadow_mask==0\n",
    "        if np.random.randint(2)==1:\n",
    "            image_hls[:,:,1][cond1] = image_hls[:,:,1][cond1]*random_bright\n",
    "        else:\n",
    "            image_hls[:,:,1][cond0] = image_hls[:,:,1][cond0]*random_bright    \n",
    "    image = cv2.cvtColor(image_hls,cv2.COLOR_HLS2RGB)\n",
    "    return image\n",
    "\n",
    "\n",
    "def preprocessImage(image):\n",
    "    \n",
    "    image = image[math.floor(160/5):(160-25), 0:320]\n",
    "    image = scipy.misc.imresize(image, (66, 200))\n",
    " \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7232\n"
     ]
    }
   ],
   "source": [
    "root = os.getcwd()\n",
    "data_folder = os.path.join(root, 'data')\n",
    "csv_file = os.path.join(root, 'data\\driving_log.csv')\n",
    "data = pd.read_csv(csv_file, usecols = ['center', 'left', 'right', 'steering'])\n",
    "\n",
    "\n",
    "image = list(zip(list(data.center), list(data.left), list(data.right)))\n",
    "steering = list(data.steering)\n",
    "\n",
    "center, steering = shuffle(image, steering)\n",
    "\n",
    "path_train, path_valid, y_train, y_valid = train_test_split(image, steering, test_size=0.1, random_state=36) \n",
    "\n",
    "print(len(y_train))\n",
    "\n",
    "\n",
    "def gen_single(path, y):\n",
    "    image_path, steer = path, y\n",
    "    dice = np.random.randint(3)\n",
    "    if dice == 0:\n",
    "        path_file = image_path[0]\n",
    "        shift_ang = 0\n",
    "    if dice == 1:\n",
    "        path_file = image_path[1]\n",
    "        shift_ang = 0.20\n",
    "    if dice == 2:\n",
    "        path_file = image_path[2]\n",
    "        shift_ang = -0.20\n",
    "    steer_plus = steer + shift_ang\n",
    "\n",
    "    image = cv2.imread(path_file)\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "    image,steer_plus = trans_image(image, steer_plus, 100)\n",
    "    image = augment_brightness_camera_images(image)\n",
    "    image = add_random_shadow(image)\n",
    "    image = preprocessImage(image)\n",
    "    image = np.array(image)\n",
    "    ind_flip = np.random.randint(2)\n",
    "    if ind_flip==0:\n",
    "        image = cv2.flip(image,1)\n",
    "        steer_plus = -steer_plus\n",
    "    return image, steer_plus\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_arrays(path, y, BATCH_SIZE = 32):\n",
    "    while 1:\n",
    "        path, y = shuffle(path, y)\n",
    "        num_examples = len(y)\n",
    "        batch_y = np.zeros(BATCH_SIZE)\n",
    "        batch_X = np.zeros((BATCH_SIZE, 66, 200, 3))\n",
    "\n",
    "        \n",
    "        for num in range(BATCH_SIZE):            \n",
    "            keeper = 0\n",
    "            while keeper == 0:\n",
    "                rand_index = np.random.randint(num_examples)\n",
    "                image, steering = gen_single(path[rand_index],y[rand_index])\n",
    "\n",
    "                if abs(steering) < 0.06:\n",
    "                    pr_val = np.random.uniform()\n",
    "                    if pr_val > 0.4:\n",
    "                        keeper = 1\n",
    "                else:\n",
    "                    keeper = 1\n",
    "            batch_X[num, :, :, :] = image\n",
    "            batch_y[num] = steering\n",
    "        yield(batch_X, batch_y)\n",
    "                        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_1 (Lambda)                (None, 66, 200, 3)    0           lambda_input_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 31, 98, 24)    1824        lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "elu_1 (ELU)                      (None, 31, 98, 24)    0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 14, 47, 36)    21636       elu_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "elu_2 (ELU)                      (None, 14, 47, 36)    0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 5, 22, 48)     43248       elu_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "elu_3 (ELU)                      (None, 5, 22, 48)     0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 3, 20, 64)     27712       elu_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "elu_4 (ELU)                      (None, 3, 20, 64)     0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 1, 18, 64)     36928       elu_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "elu_5 (ELU)                      (None, 1, 18, 64)     0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 1152)          0           elu_5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 100)           115300      flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "elu_6 (ELU)                      (None, 100)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 50)            5050        elu_6[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "elu_7 (ELU)                      (None, 50)            0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 10)            510         elu_7[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "elu_8 (ELU)                      (None, 10)            0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             11          elu_8[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 252,219\n",
      "Trainable params: 252,219\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, MaxPooling2D, AveragePooling2D, Dropout, Activation, Dense, Lambda, ELU, BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x/127.5-1, input_shape = (66, 200, 3)))\n",
    "model.add(Conv2D(24, 5, 5, subsample=(2, 2),  border_mode='valid', init = 'he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Conv2D(36, 5, 5, subsample=(2, 2),  border_mode='valid', init = 'he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Conv2D(48, 5, 5, subsample=(2, 2),  border_mode='valid', init = 'he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Conv2D(64, 3, 3, subsample=(1, 1),  border_mode='valid', init = 'he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Conv2D(64, 3, 3, subsample=(1, 1),  border_mode='valid', init = 'he_normal'))\n",
    "model.add(ELU())\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, init = 'he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Dense(50, init = 'he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Dense(10, init = 'he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Dense(1, init = 'he_normal'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_1 (Lambda)                (None, 66, 200, 3)    0           lambda_input_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 31, 98, 24)    1824        lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "elu_1 (ELU)                      (None, 31, 98, 24)    0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 14, 47, 36)    21636       elu_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "elu_2 (ELU)                      (None, 14, 47, 36)    0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 5, 22, 48)     43248       elu_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "elu_3 (ELU)                      (None, 5, 22, 48)     0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 3, 20, 64)     27712       elu_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "elu_4 (ELU)                      (None, 3, 20, 64)     0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 1, 18, 64)     36928       elu_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "elu_5 (ELU)                      (None, 1, 18, 64)     0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 1152)          0           elu_5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 100)           115300      flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "elu_6 (ELU)                      (None, 100)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 50)            5050        elu_6[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "elu_7 (ELU)                      (None, 50)            0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 10)            510         elu_7[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "elu_8 (ELU)                      (None, 10)            0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             11          elu_8[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 252,219\n",
      "Trainable params: 252,219\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, MaxPooling2D, Dropout, Activation, Dense\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "\n",
    "with open('model8.json', 'r') as f:\n",
    "    json_string = f.read()\n",
    "f.close()\n",
    "model = model_from_json(json_string)\n",
    "\n",
    "try:\n",
    "    model.load_weights('model8.h5')\n",
    "except:\n",
    "    print(\"Unexpected error\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "7232/7232 [==============================] - 343s - loss: 0.0549 - acc: 0.0000e+00 - val_loss: 0.0506 - val_acc: 0.0000e+00\n",
      "Epoch 2/8\n",
      "7232/7232 [==============================] - 244s - loss: 0.0469 - acc: 0.0000e+00 - val_loss: 0.0519 - val_acc: 0.0000e+00\n",
      "Epoch 3/8\n",
      "7232/7232 [==============================] - 214s - loss: 0.0452 - acc: 0.0000e+00 - val_loss: 0.0453 - val_acc: 0.0000e+00\n",
      "Epoch 4/8\n",
      "7232/7232 [==============================] - 201s - loss: 0.0449 - acc: 0.0000e+00 - val_loss: 0.0453 - val_acc: 0.0000e+00\n",
      "Epoch 5/8\n",
      "7232/7232 [==============================] - 191s - loss: 0.0438 - acc: 0.0000e+00 - val_loss: 0.0420 - val_acc: 0.0000e+00\n",
      "Epoch 6/8\n",
      "7232/7232 [==============================] - 183s - loss: 0.0420 - acc: 0.0000e+00 - val_loss: 0.0425 - val_acc: 0.0000e+00\n",
      "Epoch 7/8\n",
      "7232/7232 [==============================] - 183s - loss: 0.0432 - acc: 0.0000e+00 - val_loss: 0.0387 - val_acc: 0.0000e+00\n",
      "Epoch 8/8\n",
      "7232/7232 [==============================] - 190s - loss: 0.0394 - acc: 0.0000e+00 - val_loss: 0.0384 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "train_generator = generate_arrays(path_train, y_train)\n",
    "valid_generator = generate_arrays(path_valid, y_valid)\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                    samples_per_epoch = (len(y_train)//32)*32, nb_epoch = 8,\n",
    "                    validation_data = valid_generator,  nb_val_samples=len(y_valid), \n",
    "                    )\n",
    "\n",
    "json_string = model.to_json()\n",
    "\n",
    "with open('model8.json', 'w') as f:\n",
    "    f.write(json_string)\n",
    "f.close()\n",
    "try:    \n",
    "    model.save_weights('model8.h5', overwrite = True)\n",
    "except:\n",
    "    print(\"Unexpected error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
